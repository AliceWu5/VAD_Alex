/********************************************************************
*
* Copyright (c) 2017, Vanessa Feng.
*
* Rewrite from Alex-vad-ffnn which is writen by Python, we want to 
* use it to Andrews equipment which writen by Java. Because of better 
* transferring this feature， we need to rewrite it by C or C++.
*
*********************************************************************/
#include "ffnn.h"

FFNNVADGeneral::FFNNVADGeneral(int sample_rate, int frame_size, int frame_shift, bool usehamming, double preem_coef, int num_chans, int ceplifter, int num_ceps, int n_last_frames, int n_prev_frames, int lo_freq, int hi_freq, bool mel_banks_only){
	
	samplerate = sample_rate;
	framesize = frame_size;
	frameshift = frame_shift;
	preemcoef = preem_coef;	
	filter_length = 2;	
	numchans = num_chans;	
	numceps = num_ceps;
	lofreq = lo_freq;
	hifreq = hi_freq;	
	n_frames = n_last_frames+n_prev_frames;
	mel_only = mel_banks_only;

	signal_in = new double[framesize];
	for(int i=0;i<framesize;i++){
		signal_in[i]=0.0;
	}
}

FFNNVADGeneral::~FFNNVADGeneral()
{
	delete[] signal_in;
}

	
void FFNNVADGeneral::decide(char* input,int sound_size)
{
	short* __restrict signal = (short *)input;
	int num_frame = 0;
	audio_recorded.reserve(framesize);
	/*Error:只能添加signal[0]，用指针不能使每项/32767.0
	short *data = &signal[0];
	audio_recorded_in.push_back(*data); */
	
	MFCCFrontEnd front_end(samplerate, framesize, preemcoef, numchans, numceps, n_frames, lofreq, hifreq, mel_only);

	front_end.getMFCCs();
	
	FFNNetwork ffnn;
	ffnn.load("./vad_model.json");
	cout << "Now input the vadio to ffnn.." << endl;
	cout << "-----------------------------------------------------" << endl;
	
	while(sound_size/(2*frameshift)-num_frame){
		for(int i=0; i<framesize; i++){
			signal_in[i] = double(signal[i]);
			//signal_in[i] = double(signal[i])/32767.0;
			audio_recorded.push_back(signal_in[i]);
			//cout << signal_in[i] << endl;
		
		}
		/*for(int i=0; i<framesize; i++){
			cout << audio_recorded[i] << endl;
		}
		*/
		
		mfcc = front_end.param(audio_recorded, mfcc); 

		/********************* FFNN  **************************/
		/*prob_result = [prob_sil,prob_speech]
		* 源python:[[prob_sil,prob_speech]]
		* 函数:vector<double> predict_normalise(vector<double> mfcc)  
		* 函数返回数组 predict_normalise返回r：static double r[2];
		* prob_result: 指向数组[prob_sil,prob_speech]的指针
		*/

		if(mfcc.empty()){
			cout << "Error: Input null to ffnn\n";
			exit(0);
		}
		prob_result = ffnn.predict_normalise(&mfcc[0]);
		//cout << prob_result[0]<<"\t"<<prob_result[1] <<endl;
		double prob_sil = *prob_result;
		double prob_speech = *(prob_result+1);
	
		log_probs_speech.push_back(log(prob_speech));
		log_probs_sil.push_back(log(prob_sil));
		while (log_probs_speech.size()>2)
		{
			log_probs_speech.pop_front();
		}
		while (log_probs_sil.size()>2)
		{
			log_probs_sil.pop_front();
		}	
	
		log_prob_speech_avg = 0.0;
		for(int i=0;i<log_probs_speech.size();i++)
		{
			log_prob_speech_avg += log_probs_speech[i] - logsumexp(log_probs_speech[i],log_probs_sil[i]);
		}
	
		log_prob_speech_avg /= log_probs_speech.size();
	
		prob_speech_avg = exp(log_prob_speech_avg);
		cout << "frame: " << num_frame << "\ttime: " << num_frame*(frameshift/(float)samplerate) << "\tlast_decision: " << prob_speech_avg << endl;
		
		mfcc.clear();
	 	signal+=frameshift; //语音数据指针向后移frameshift
		num_frame++;
		audio_recorded.clear();
	}	
}



