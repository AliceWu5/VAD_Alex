#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import cPickle as pickle
import random
import copy

import theano
from theano import function
from theano import tensor as T
import numpy.random as rng

from collections import deque
import numpy as np
from scipy.misc import logsumexp
from scipy.fftpack import dct
import struct

from math import log


######################################## TheanoFFNN ######################################
rng.seed(0)
class TheanoFFNN(object):
    """ Implements simple feed-forward neural network with:

      -- input layer - activation function linear
      -- hidden layers - activation function tanh
      -- output layer - activation function softmax
    """
    def __init__(self, n_inputs = 0, n_hidden_units = 0, n_hidden_layers = 0, n_outputs = 0,
                 training_set_x = None, training_set_y = None, prev_frames = 0, next_frames = 0, amplify_center_frame = 1.0,
                 batch_size = 0, hidden_activation = 'tanh', weight_l2 = 1e-6):
        self.n_inputs = n_inputs

        if hidden_activation == 'tanh':
            self.hidden_activation = T.tanh
        elif hidden_activation == 'sigmoid':
            self.hidden_activation = T.nnet.sigmoid
        elif hidden_activation == 'softplus':
            self.hidden_activation = T.nnet.softplus
        elif hidden_activation == 'relu':
            self.hidden_activation = lambda x: T.maximum(0, x)
        else:
            raise NotImplementedError

        self.n_outputs = n_outputs
        self.weight_l2 = weight_l2

        self.training_set_x = training_set_x
        self.training_set_y = training_set_y

        self.prev_frames = prev_frames
        self.next_frames = next_frames
        self.batch_size = batch_size

        amp_min = 1.0/amplify_center_frame
        amp_max = 1.0
         
        amp_prev = [amp_min + (float(i) / self.prev_frames) * (amp_max - amp_min) for i in range(0, self.prev_frames)]
        amp_next = [amp_min + (float(i) / self.next_frames) * (amp_max - amp_min) for i in range(0, self.next_frames)]
        self.amp = amp_prev + [amp_max,] + list(reversed(amp_next))
        self.amp_vec = np.repeat(self.amp, n_inputs / (self.prev_frames + 1 + self.next_frames))

        if n_inputs:
            self.build_model(n_hidden_units, n_hidden_layers)

    def build_model(self, n_hidden_units, n_hidden_layers, old_params = None):
        # Model definition.
        x = T.fmatrix('X')
        y = x

        # Keep model params here.
        self.params = []  

        # Build the layered neural network.
        if not old_params:
            self.n_hidden = [n_hidden_units,]*n_hidden_layers
        else:
            self.n_hidden = self.n_hidden + [n_hidden_units,]*n_hidden_layers
        
        activations = [self.hidden_activation,]*len(self.n_hidden)
        activations.extend([T.nnet.softmax,]) # NOTE: The last function goes to the output layer.

        assert len(self.n_hidden) + 1 == len(activations)
        
        layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]
        
        # Iterate over pairs of adjacent layers.
        for i, (n1, n2, act) in enumerate(zip(layers[:-1], layers[1:], activations)):
            #print i, n1, n2, act
            
            if old_params and (2*i < len(old_params)):
                #print "using old params"
                # init an existing layer
                w = theano.shared(old_params[2*i], 'W%d' % i, borrow=True)
                b = theano.shared(old_params[2*i+1], 'b%d' % (i + 1))
            else:
                #print "sampling new params"
                w = theano.shared(
                                   np.asarray(rng.uniform(
                                                          low=-np.sqrt(6. / (n1 + n2)),
                                                          high=np.sqrt(6. / (n1 + n2)),
                                                          size=(n1, n2)),
                                              dtype=np.float32),
                                   'W%d' % i, borrow=True)
                b = theano.shared(np.zeros(n2, dtype=np.float32), 'b%d' % (i + 1))
            self.params.append(w)
            self.params.append(b)

            y = act(T.dot(y, w) + b)
        print "shape of y",y.size()
	print "shape of [x]",[x].size()
        self.f_y = function([x], y) # PREDICTION FUNCTION

        # Define the loss function.
        true_y = T.ivector('true_Y')  # The desired output vector.
        loss = T.log(y[T.arange(y.shape[0]), true_y])  # log-likelihood.
        loss = T.mean(loss)                            # MEAN log-likelihood.

        # Add regularization.
        l2 = 0
        for p in self.params:
            l2 += (p**2).sum()
        loss -= self.weight_l2 * l2

        self.f_loss = function([x, true_y], loss, allow_input_downcast=True)

        # Derive the gradients for the parameters.
        g_loss = T.grad(loss, wrt=self.params)
        self.f_g_loss = function([x, true_y], g_loss)

        # Create a training function for maximization
        updates = []
        learning_rate = T.fscalar()
        for p, g in zip(self.params, g_loss):
            updates.append((p, p + learning_rate * g))

        self.f_train_ret_loss = function([x, true_y, learning_rate], loss, updates = updates, allow_input_downcast=True)


    def add_hidden_layer(self, n_hidden_units):    
        ''' It is like a building a complete network, you have to just initialise the network using 
        the parameters from the previous network.
        ''' 
        
        # Keep model params here.
        old_params = [p.get_value() for p in self.params]

        # Remove the last layer parameters
        old_params = old_params[:-2]

        self.build_model(n_hidden_units, 1, old_params)

    def set_input_norm(self, m, std):
        self.input_m = m
        self.input_std = std

    def set_params(self, params):
        """ Set new NN params and build the network model.
        """
        self.input_m, \
        self.input_std, \
        old_params, \
        self.n_hidden, \
        self.hidden_activation, \
        self.n_inputs, \
        self.n_outputs, \
        self.weight_l2, \
        self.prev_frames, \
        self.next_frames, \
        self.batch_size, \
        self.amp, \
        self.amp_vec = params

        self.build_model(0, 0, old_params = old_params)
                
    def get_params(self):
        """ Get all NN params.
        """
        params = (self.input_m, 
		            self.input_std, 
		            [p.get_value() for p in self.params],
		            self.n_hidden,
		            self.hidden_activation,
		            self.n_inputs,
		            self.n_outputs,
		            self.weight_l2,
		            self.prev_frames,
		            self.next_frames,
		            self.batch_size,
		            self.amp,
		            self.amp_vec,
		            )
        return params
    
    def load(self, file_name):
        """ Loads saved NN.

        :param file_name: file name of the saved NN
        :return: None
        """
        with open(file_name, "rb") as f:
            self.set_params(pickle.load(f))

    def save(self, file_name):
        """ Saves the NN into a file.

        :param file_name: name of the file where the NN will be saved
        :return: None
        """
        with open(file_name, "wb") as f:
            pickle.dump(self.get_params(), f)
                
    def predict(self, data_x, batch_size = 0, prev_frames = 0, next_frames = 0, data_y = None):
        if not batch_size:
            if prev_frames or next_frames:
                mx = self.frame_multiply_x(data_x, prev_frames, next_frames)
            else:
                mx = data_x
                
            if data_y != None:
                my = self.frame_multiply_y(data_y, prev_frames, next_frames)
                return self.f_y(mx), my
                                
            return self.f_y(mx)
        else:
            res = []
            resy = []
            for i in range(0, len(data_x), batch_size):
                if prev_frames or next_frames:
                    mx = self.frame_multiply_x(data_x[i:i+batch_size], prev_frames, next_frames)
                else:
                    mx = data_x[i:i+batch_size]
                    
                if data_y != None:
                    my = self.frame_multiply_y(data_y[i:i+batch_size], prev_frames, next_frames)
                    resy.append(my)
                    
                res.append(self.f_y(mx))
                
            if data_y != None:
                return np.vstack(res), np.concatenate(resy) 
                                
            return np.vstack(res)

    def predict_normalise(self, input):
        input -= self.input_m
        input /= self.input_std
        input *= self.amp_vec
        
        return self.predict(input)
        
    def frame_multiply_x(self, x, prev_frames, next_frames):
        rows = [(c, c + len(x) - (self.prev_frames + 1 + self.next_frames)) for c in range(0, self.prev_frames + 1 + self.next_frames)]
         
        mx = np.hstack([a*x[l:r] for a, (l, r) in zip(self.amp, rows)])
        return mx

    def frame_multiply_y(self, y, prev_frames, next_frames):
        my = y[prev_frames:len(y) - 1 - next_frames]
        return my

    def train(self, method = 'fixedlr', n_iters = 1, learning_rate = 0.1):
        # Do batch-gradient descent to learn the parameters.

        if self.batch_size > 0 and self.batch_size <= len(self.training_set_x):
            n_minibatches = int(len(self.training_set_x) / self.batch_size)
        else:
            n_minibatches = 1
            batch_size = len(self.training_set_x)
            
        m_minibatches = n_minibatches/10
        if m_minibatches <= 0:
            m_minibatches = 1
            
        if 'fixedlr' in method:
            print 'Minibatch size:', self.batch_size, '# minibatches:', n_minibatches, "# total data:", len(self.training_set_x)
            for ni in range(n_iters):
                for m in random.sample(range(n_minibatches), n_minibatches):
                    mini_x = self.training_set_x[m*self.batch_size:(m+1)*self.batch_size]
                    mini_y = self.training_set_y[m*self.batch_size:(m+1)*self.batch_size]

                    if self.prev_frames or self.next_frames:
                        mini_x = self.frame_multiply_x(mini_x, self.prev_frames, self.next_frames)
                        mini_y = self.frame_multiply_y(mini_y, self.prev_frames, self.next_frames)
                    
                    log_lik = self.f_train_ret_loss(mini_x, mini_y, learning_rate)
										
                    #log_lik = - self.f_train2(m, learning_rate)

                    if (m % m_minibatches) == 0:
                        print "iteration (%d)" % ni, "minibatch (%d)" % m, "log likelihood %.4f" % log_lik
        else:
            print "Unknown update method"
            return


######################################## MFCCFrontEnd ######################################

class MFCCFrontEnd:
    """This is an a CLOSE approximation of MFCC coefficients computed by the HTK.

    The frame size should be a number of power of 2.

    TODO: CMN is not implemented. It should normalise only teh cepstrum, not the delta or acc coefficients.

    It was not tested to give exactly the same results the HTK. As a result,
    it should not be used in conjunction with models trained on speech
    parametrised with the HTK.

    Over all it appears that this implementation of MFCC is worse than the one from the HTK.
    On the VAD task, the HTK features score 90.8% and the this features scores only 88.7%.
    """

    def __init__(self, sourcerate=16000, framesize=512,
                 usehamming=True, preemcoef=0.97,
                 numchans=26, ceplifter=22, numceps=12,
                 enormalise=True, zmeansource=True, usepower=True, usec0=True, usecmn=False,
                 usedelta=True, useacc=True, n_last_frames = 0,
                 lofreq=125, hifreq=3800, mel_banks_only = False):
        self.sourcerate = sourcerate
        self.framesize = framesize
        self.usehamming = usehamming
        self.preemcoef = preemcoef
        self.numchans = numchans
        self.ceplifter = ceplifter
        self.enormalise = enormalise
        self.zmeansource = zmeansource
        self.usepower = usepower
        self.usec0 = usec0
        self.usecmn = usecmn
        self.usedelta = usedelta
        self.useacc = useacc
        self.numceps = numceps
        self.lofreq = lofreq
        self.hifreq = hifreq
        self.mel_banks_only = mel_banks_only

        self.prior = 0.0

        self.n_last_frames = n_last_frames
        self.mfcc_queue = deque(maxlen=4 + n_last_frames)
        self.mfcc_delta_queue = deque(maxlen=4 + n_last_frames)

        self.init_hamming()
        self.init_mel_filter_bank()
        self.init_cep_liftering_weights()

    def freq_to_mel(self, freq):
        return 1127 * np.log(1.0 + freq / 700.0)

    def mel_to_freq(self, mel):
        return 700 * (np.exp(mel / 1127) - 1.0)

    def init_hamming(self):
        self.hamming = np.hamming(self.framesize)

    def init_mel_filter_bank(self):
        """Initialise the triangular mel freq filters."""

        minMel = self.freq_to_mel(self.lofreq)
        maxMel = self.freq_to_mel(self.hifreq)

#    print "MM", minMel, "MM", maxMel

        # Create a matrix for triangular filters, one row per filter
        filterMatrix = np.zeros((self.numchans, self.framesize / 2 + 1))

        melRange = np.array(xrange(self.numchans + 2))
#    print "MR", melRange

        melCenterFilters = melRange * (maxMel - minMel) / (
            self.numchans + 1) + minMel
#    print "MCF", melCenterFilters

        dfreq = self.sourcerate / self.framesize
        # each array index represent the center of each triangular filter
        centerIndex = np.array(
            np.round(self.mel_to_freq(melCenterFilters) / dfreq), int)
#    print "CI", centerIndex

        for i in xrange(self.numchans):
            start, centre, end = centerIndex[i:i + 3]
            k1 = np.float32(centre - start)
            k2 = np.float32(end - centre)
            up = (np.array(xrange(start, centre)) - start) / k1
            down = (end - np.array(xrange(centre, end))) / k2

            filterMatrix[i][start:centre] = up
            filterMatrix[i][centre:end] = down

        self.mel_filter_bank = filterMatrix.transpose()
#    print "SMFB", self.mel_filter_bank.shape

    def init_cep_liftering_weights(self):
        cep_lift_weights = np.zeros((self.numceps, ))
        a = np.pi / self.ceplifter
        b = self.ceplifter / 2.0
        for i in range(self.numceps):
            cep_lift_weights[i] = 1.0 + b * np.sin(i * a)

        self.cep_lift_weights = cep_lift_weights

    def preemphasis(self, frame):
        out_frame = np.zeros_like(frame)
        out_frame[0] = frame[0] - self.preemcoef * self.prior
        for i in range(1, len(frame)):
            out_frame[i] = frame[i] - self.preemcoef * frame[i - 1]

        self.prior = frame[-1]
	
        return out_frame

    def param(self, frame):
        """Compute the MFCC coefficients in a way similar to the HTK."""
        # zero mean
        if self.zmeansource:
            frame = frame - np.mean(frame)
        # preemphasis
        frame = self.preemphasis(frame)
        # apply hamming window
        if self.usehamming:
            frame = self.hamming * frame

        complex_spectrum = np.fft.rfft(frame)
#    print "LCS", len(complex_spectrum)
        power_spectrum = complex_spectrum.real * complex_spectrum.real + \
            complex_spectrum.imag * complex_spectrum.imag
        # compute only power spectrum if required
        if not self.usepower:
            power_spectrum = np.sqrt(power_spectrum)

#    print "SPS",power_spectrum.shape
        mel_spectrum = np.dot(power_spectrum, self.mel_filter_bank)
        # apply mel floor
        for i in range(len(mel_spectrum)):
            if mel_spectrum[i] < 1.0:
                mel_spectrum[i] = 1.0
        mel_spectrum = np.log(mel_spectrum)
        
        if self.mel_banks_only:
            mfcc = mel_spectrum
            self.mfcc_queue.append(mel_spectrum)
        else:
            cepstrum = dct(mel_spectrum, type=2, norm='ortho')
            c0 = cepstrum[0]
            htk_cepstrum = cepstrum[1:self.numceps + 1]
            # cepstral liftering
            cep_lift_mfcc = self.cep_lift_weights * htk_cepstrum

            if self.usec0:
                mfcc = np.append(cep_lift_mfcc, c0)
            else:
                mfcc = cep_lift_mfcc

            # compute delta and acceleration coefficients if requested
            self.mfcc_queue.append(mfcc)

#        print len(self.mfcc_queue)

            if self.usedelta:
#      print "LMQ", len(self.mfcc_queue)
                if len(self.mfcc_queue) >= 2:
                    delta = np.zeros_like(mfcc)
                    for i in range(1, len(self.mfcc_queue)):
                        delta += self.mfcc_queue[i] - self.mfcc_queue[i - 1]
                    delta /= len(self.mfcc_queue) - 1

                    self.mfcc_delta_queue.append(delta)
                else:
                    delta = np.zeros_like(mfcc)

            if self.useacc:
                if len(self.mfcc_delta_queue) >= 2:
                    acc = np.zeros_like(mfcc)
                    for i in range(1, len(self.mfcc_delta_queue)):
                        acc += self.mfcc_delta_queue[i] - \
                            self.mfcc_delta_queue[i - 1]
                    acc /= len(self.mfcc_delta_queue) - 1
                else:
                    acc = np.zeros_like(mfcc)

            if self.usedelta:
                mfcc = np.append(mfcc, delta)
            if self.useacc:
                mfcc = np.append(mfcc, acc)

        for i in range(self.n_last_frames):
            if len(self.mfcc_queue) > i + 1 :
                mfcc = np.append(mfcc, self.mfcc_queue[-1-i-1])
            else:
                mfcc = np.append(mfcc, np.zeros_like(self.mfcc_queue[-1]))

        return mfcc.astype(np.float32)

######################################## FFNNVADGeneral ######################################

class FFNNVADGeneral(object):
    """ This is implementation of a FFNN based voice activity detector.

    It only implements decisions whether input frame is speech of non speech.
    It returns the posterior probability of speech for N last input frames.
    """
    def __init__(self, model, filter_length, sample_rate, framesize, frameshift,
                 usehamming, preemcoef, numchans,  ceplifter, numceps,
                 enormalise, zmeansource, usepower, usec0, usecmn, usedelta,
                 useacc, n_last_frames, n_prev_frames, lofreq, hifreq,
                 mel_banks_only):
        self.audio_recorded_in = []

        self.ffnn = TheanoFFNN()
        self.ffnn.load(model)

        self.log_probs_speech = deque(maxlen=filter_length)
        self.log_probs_sil = deque(maxlen=filter_length)

        self.last_decision = 0.0


        self.front_end = MFCCFrontEnd(
            sample_rate, framesize,
            usehamming, preemcoef,
            numchans, ceplifter,
            numceps, enormalise,
            zmeansource, usepower,
            usec0, usecmn,
            usedelta, useacc,
            n_last_frames + n_prev_frames,
            lofreq, hifreq,
            mel_banks_only)

        self.framesize = framesize
        self.frameshift = frameshift

    def decide(self, data):
        """Processes the input frame whether the input segment is speech or non speech.

        The returned values can be in range from 0.0 to 1.0.
        It returns 1.0 for 100% speech segment and 0.0 for 100% non speech segment.
        """
        
	# print(struct.calcsize('%dh' % (len(data) / 2, )))
        data = struct.unpack('%dh' % (len(data) / 2, ), data)
	
        self.audio_recorded_in.extend(data)
        num_frame = 0
        while len(self.audio_recorded_in) > self.framesize:
            num_frame += 1
            frame = self.audio_recorded_in[:self.framesize]
            self.audio_recorded_in = self.audio_recorded_in[self.frameshift:]

            mfcc = self.front_end.param(frame)
	    
            # prob_sil, prob_speech = self.ffnn.predict_normalise(mfcc.reshape(1,len(mfcc)))[0]
            prob_sil, prob_speech = self.ffnn.predict_normalise(mfcc)[0]
            # print prob_sil, prob_speech

            self.log_probs_speech.append(log(prob_speech))
            self.log_probs_sil.append(log(prob_sil))

            log_prob_speech_avg = 0.0
            for log_prob_speech, log_prob_sil in zip(self.log_probs_speech, self.log_probs_sil):
                log_prob_speech_avg += log_prob_speech - logsumexp([log_prob_speech, log_prob_sil])
            
            log_prob_speech_avg /= len(self.log_probs_speech)

            prob_speech_avg = np.exp(log_prob_speech_avg)
	    
            print 'frame: ',num_frame,'time: ',num_frame*160/16000.0,'prob_speech_avg: %5.3f' % prob_speech_avg

            self.last_decision = prob_speech_avg
	    
        # returns a speech / non-speech decisions
        return self.last_decision



